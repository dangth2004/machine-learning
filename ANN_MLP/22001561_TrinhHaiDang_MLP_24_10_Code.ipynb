{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:38.182384Z",
     "start_time": "2024-10-24T04:43:37.211834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ],
   "id": "14da1b9aa8644aa3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:38.192918Z",
     "start_time": "2024-10-24T04:43:38.185699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "x_data = iris.data[:, :4]\n",
    "y_data = iris.target\n",
    "C = 3  # Number of classes"
   ],
   "id": "cdbb479cae228c9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:38.312727Z",
     "start_time": "2024-10-24T04:43:38.309724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the training set and validation set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data,\n",
    "                                                    test_size=0.2, random_state=42)"
   ],
   "id": "74b6eb32dad4f1a8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:38.329489Z",
     "start_time": "2024-10-24T04:43:38.325121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(V):\n",
    "    e_V = np.exp(V - np.max(V, axis=0, keepdims=True))\n",
    "    Z = e_V / e_V.sum(axis=0)\n",
    "    return Z\n",
    "\n",
    "\n",
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return -np.sum(Y * np.log(Yhat)) / Y.shape[0]\n",
    "\n",
    "def ann(d0, d1, d2):\n",
    "    # ANN\n",
    "    h = d1  # size of hidden layer\n",
    "    C = d2\n",
    "    # initialize parameters randomly\n",
    "    W1 = 0.01 * np.random.randn(d0, d1)\n",
    "    b1 = np.zeros((d1, 1))\n",
    "    W2 = 0.01 * np.random.randn(d1, d2)\n",
    "    b2 = np.zeros((d2, 1))\n",
    "    \n",
    "    N = x_data.shape[1]\n",
    "    eta = 1  # learning rate\n",
    "    \n",
    "    # Gradient descent method\n",
    "    for i in range(10000):\n",
    "        ## Feedforward\n",
    "        Z1 = np.dot(W1.T, x_train.T) + b1\n",
    "        A1 = np.maximum(Z1, 0)\n",
    "        Z2 = np.dot(W2.T, A1) + b2\n",
    "        Yhat = softmax(Z2)\n",
    "    \n",
    "        # print loss after each 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            # compute the loss: average cross-entropy loss\n",
    "            loss = cost(y_train, Yhat)\n",
    "            print(\"iter %d, loss: %f\" % (i, loss))\n",
    "    \n",
    "        # backpropagation\n",
    "        E2 = (Yhat - y_train) / N\n",
    "        dW2 = np.dot(A1, E2.T)\n",
    "        db2 = np.sum(E2, axis=1, keepdims=True)\n",
    "        E1 = np.dot(W2, E2)\n",
    "        E1[Z1 <= 0] = 0  # gradient of ReLU\n",
    "        dW1 = np.dot(x_train.T, E1.T)\n",
    "        db1 = np.sum(E1, axis=1, keepdims=True)\n",
    "    \n",
    "        # Gradient Descent update\n",
    "        W1 += -eta * dW1\n",
    "        b1 += -eta * db1\n",
    "        W2 += -eta * dW2\n",
    "        b2 += -eta * db2\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ],
   "id": "d9bfa4b5948bcec0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:39.350693Z",
     "start_time": "2024-10-24T04:43:38.342679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "W1, b1, W2, b2 = ann(4, 100, 3)\n",
    "Z1 = np.dot(W1.T, x_train.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "acc = 100 * np.mean(predicted_class == y_train)\n",
    "print('training accuracy: %.2f %%' % (acc))"
   ],
   "id": "3f20f9c7dd3b292b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 3.268375\n",
      "iter 1000, loss: nan\n",
      "iter 2000, loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trinh Hai Dang\\AppData\\Local\\Temp\\ipykernel_10296\\1895195136.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  e_V = np.exp(V - np.max(V, axis=0, keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000, loss: nan\n",
      "iter 4000, loss: nan\n",
      "iter 5000, loss: nan\n",
      "iter 6000, loss: nan\n",
      "iter 7000, loss: nan\n",
      "iter 8000, loss: nan\n",
      "iter 9000, loss: nan\n",
      "training accuracy: 33.33 %\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:39.384759Z",
     "start_time": "2024-10-24T04:43:39.370639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Multinomial Logistic Regression (Softmax) approach\n",
    "# Train model\n",
    "start_train_softmax = time.perf_counter() # start of training time\n",
    "softmax_approach = LogisticRegression()\n",
    "softmax_approach.fit(x_train, y_train)\n",
    "end_train_softmax = time.perf_counter() # end of training time\n",
    "# Predict results\n",
    "start_test_softmax = time.perf_counter() # start of predicting time\n",
    "y_pred_softmax = softmax_approach.predict(x_test)\n",
    "end_test_softmax = time.perf_counter() # start of predicting time\n",
    "# Print accuracy, recall, precision score\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred_softmax)}')\n",
    "print(f'Training time: {end_train_softmax - start_train_softmax}')\n",
    "print(f'Predicting time: {end_test_softmax - start_test_softmax}')"
   ],
   "id": "774eb4db1c67958d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 1.0\n",
      "Training time: 0.009376399997563567\n",
      "Predicting time: 0.00022139999782666564\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:39.411500Z",
     "start_time": "2024-10-24T04:43:39.406030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Naive Bayes approach\n",
    "# Train model\n",
    "start_train_naive = time.perf_counter() # start of training time\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(x_train, y_train)\n",
    "end_train_naive = time.perf_counter() # end of training time\n",
    "# Predict result\n",
    "start_test_naive = time.perf_counter() # start of predicting time\n",
    "y_pred_naive = naive_bayes.predict(x_test)\n",
    "end_test_naive = time.perf_counter() # start of predicting time\n",
    "# Print accuracy, recall, precision score\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred_naive)}')\n",
    "print(f'Training time: {end_train_naive - start_train_naive}')\n",
    "print(f'Predicting time: {end_test_naive - start_test_naive}')"
   ],
   "id": "713ce974fe915fc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 1.0\n",
      "Training time: 0.0011805000031017698\n",
      "Predicting time: 0.00021189999824855477\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:40.444651Z",
     "start_time": "2024-10-24T04:43:39.555408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "W1_75, b1_75, W2_75, b2_75 = ann(4, 75, 3)\n",
    "Z1_75 = np.dot(W1_75.T, x_train.T) + b1_75\n",
    "A1_75 = np.maximum(Z1_75, 0)\n",
    "Z2_75 = np.dot(W2_75.T, A1_75) + b2_75\n",
    "predicted_class_75 = np.argmax(Z2_75, axis=0)\n",
    "acc_75 = 100 * np.mean(predicted_class_75 == y_train)\n",
    "print('training accuracy: %.2f %%' % (acc_75))"
   ],
   "id": "d53924e4821953c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 3.268379\n",
      "iter 1000, loss: nan\n",
      "iter 2000, loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trinh Hai Dang\\AppData\\Local\\Temp\\ipykernel_10296\\1895195136.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  e_V = np.exp(V - np.max(V, axis=0, keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000, loss: nan\n",
      "iter 4000, loss: nan\n",
      "iter 5000, loss: nan\n",
      "iter 6000, loss: nan\n",
      "iter 7000, loss: nan\n",
      "iter 8000, loss: nan\n",
      "iter 9000, loss: nan\n",
      "training accuracy: 33.33 %\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T04:43:41.207201Z",
     "start_time": "2024-10-24T04:43:40.470908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "W1_50, b1_50, W2_50, b2_50 = ann(4, 50, 3)\n",
    "Z1_50 = np.dot(W1_50.T, x_train.T) + b1_50\n",
    "A1_50 = np.maximum(Z1_50, 0)\n",
    "Z2_50 = np.dot(W2_50.T, A1_50) + b2_50\n",
    "predicted_class_50 = np.argmax(Z2_50, axis=0)\n",
    "acc_50 = 100 * np.mean(predicted_class_50 == y_train)\n",
    "print('training accuracy: %.2f %%' % (acc_50))"
   ],
   "id": "98490bddbebb2dd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 3.268384\n",
      "iter 1000, loss: nan\n",
      "iter 2000, loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trinh Hai Dang\\AppData\\Local\\Temp\\ipykernel_10296\\1895195136.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  e_V = np.exp(V - np.max(V, axis=0, keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000, loss: nan\n",
      "iter 4000, loss: nan\n",
      "iter 5000, loss: nan\n",
      "iter 6000, loss: nan\n",
      "iter 7000, loss: nan\n",
      "iter 8000, loss: nan\n",
      "iter 9000, loss: nan\n",
      "training accuracy: 33.33 %\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
